{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "vNNOOnw2qK_E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4bmvLIco8vz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Datasets"
      ],
      "metadata": {
        "id": "FZlb6EgQRbqe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09ROoZg7o-gD",
        "outputId": "3ab19394-e939-4a42-f4f8-d3c03567553d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # Datasets are fetched from google drive(previously stored)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH1XRQ6zpbiI"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(\"/content/drive/My Drive/Fraud Detection/dataset1_train.csv\")\n",
        "df2 = pd.read_csv(\"/content/drive/My Drive/Fraud Detection/dataset2_train.csv\")\n",
        "df3 = pd.read_csv(\"/content/drive/My Drive/Fraud Detection/dataset3_train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "Uy7ZpMYBRoH2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FTUl-vsqB3i"
      },
      "outputs": [],
      "source": [
        "df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmZSpwSz0InZ"
      },
      "outputs": [],
      "source": [
        "df2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ0wAA9z0RZD"
      },
      "outputs": [],
      "source": [
        "df3.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7F6kETX4V-H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL7T7NcG4aQN"
      },
      "outputs": [],
      "source": [
        "# Split each dataset into train and test\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(df1.drop(['class'],axis=1), df1['class'], test_size=0.2, stratify=df1['class'], random_state=42)\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(df2.drop(['Class'],axis=1), df2['Class'], test_size=0.2, stratify=df2['Class'], random_state=42)\n",
        "X3_train, X3_test, y3_train, y3_test = train_test_split(df3.drop(['is_fraud'],axis=1), df3['is_fraud'], test_size=0.2, stratify=df3['is_fraud'], random_state=42)\n",
        "\n",
        "# Normalize features\n",
        "scaler1, scaler2, scaler3 = StandardScaler(), StandardScaler(), StandardScaler()\n",
        "X1_train, X1_test = scaler1.fit_transform(X1_train), scaler1.transform(X1_test)\n",
        "X2_train, X2_test = scaler2.fit_transform(X2_train), scaler2.transform(X2_test)\n",
        "X3_train, X3_test = scaler3.fit_transform(X3_train), scaler3.transform(X3_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X1_train, X1_test = torch.tensor(X1_train, dtype=torch.float32), torch.tensor(X1_test, dtype=torch.float32)\n",
        "X2_train, X2_test = torch.tensor(X2_train, dtype=torch.float32), torch.tensor(X2_test, dtype=torch.float32)\n",
        "X3_train, X3_test = torch.tensor(X3_train, dtype=torch.float32), torch.tensor(X3_test, dtype=torch.float32)\n",
        "y1_train, y1_test = torch.tensor(y1_train.to_numpy(), dtype=torch.float32), torch.tensor(y1_test.to_numpy(), dtype=torch.float32)\n",
        "y2_train, y2_test = torch.tensor(y2_train.to_numpy(), dtype=torch.float32), torch.tensor(y2_test.to_numpy(), dtype=torch.float32)\n",
        "y3_train, y3_test = torch.tensor(y3_train.to_numpy(), dtype=torch.float32), torch.tensor(y3_test.to_numpy(), dtype=torch.float32)\n",
        "\n",
        "# Create PyTorch DataLoaders\n",
        "batch_size = 32\n",
        "train_loader1 = DataLoader(TensorDataset(X1_train, y1_train), batch_size=batch_size, shuffle=True)\n",
        "train_loader2 = DataLoader(TensorDataset(X2_train, y2_train), batch_size=batch_size, shuffle=True)\n",
        "train_loader3 = DataLoader(TensorDataset(X3_train, y3_train), batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training of Autoencoder and Extracting Encoded Features"
      ],
      "metadata": {
        "id": "4Fly0OqBRw-2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "komTmXso2jUX"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple autoencoder\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, encoding_dim=16):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, encoding_dim)  # Compressed representation\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, input_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vdeDlYW2stp"
      },
      "outputs": [],
      "source": [
        "# Function to train an autoencoder\n",
        "def train_autoencoder(autoencoder, X, epochs=15, lr=0.001):\n",
        "    optimizer = optim.Adam(autoencoder.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        encoded, decoded = autoencoder(X)\n",
        "        loss = loss_fn(decoded, X)  # Reconstruction loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "# Train autoencoders on training sets\n",
        "autoencoder1 = train_autoencoder(Autoencoder(X1_train.shape[1]), X1_train)\n",
        "autoencoder2 = train_autoencoder(Autoencoder(X2_train.shape[1]), X2_train)\n",
        "autoencoder3 = train_autoencoder(Autoencoder(X3_train.shape[1]), X3_train)\n",
        "\n",
        "# Extract encoded features for train and test sets\n",
        "X1_train_encoded = autoencoder1.encoder(X1_train)\n",
        "X1_test_encoded = autoencoder1.encoder(X1_test)\n",
        "\n",
        "X2_train_encoded = autoencoder2.encoder(X2_train)\n",
        "X2_test_encoded = autoencoder2.encoder(X2_test)\n",
        "\n",
        "X3_train_encoded = autoencoder3.encoder(X3_train)\n",
        "X3_test_encoded = autoencoder3.encoder(X3_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OFatlW69WnP"
      },
      "outputs": [],
      "source": [
        "# Combine tensors along the first dimension (rows)\n",
        "X_test_combined = torch.cat((X1_test_encoded, X2_test_encoded, X3_test_encoded), dim=0)\n",
        "y_test_combined = torch.cat((y1_test, y2_test, y3_test), dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgVYfD5kIhZg"
      },
      "outputs": [],
      "source": [
        "client1_train_loader = DataLoader(TensorDataset(X1_train_encoded, y1_train), batch_size=batch_size, shuffle=True)\n",
        "client2_train_loader = DataLoader(TensorDataset(X2_train_encoded, y2_train), batch_size=batch_size, shuffle=True)\n",
        "client3_train_loader = DataLoader(TensorDataset(X3_train_encoded, y3_train), batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "2-Xj7TJxR-Ll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYtkl7C7JSBF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import copy\n",
        "\n",
        "# Define the Multi-Input Fraud Detection Model\n",
        "class FraudDetectionModel(nn.Module):\n",
        "    def __init__(self, input_dims):  # input_dims = [dim1, dim2, dim3] (Different for each client)\n",
        "        super(FraudDetectionModel, self).__init__()\n",
        "        self.num_clients = len(input_dims)\n",
        "\n",
        "        # Create separate input layers for each client's feature space\n",
        "        self.input_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(input_dim, 32),\n",
        "                nn.ReLU()\n",
        "            ) for input_dim in input_dims\n",
        "        ])\n",
        "\n",
        "        # Final classifier after aggregation\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(32 * self.num_clients, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x_list):\n",
        "        # Process each client's input separately\n",
        "        processed_features = [self.input_layers[i](x_list[i]) for i in range(self.num_clients)]\n",
        "\n",
        "        # Concatenate processed features before classification\n",
        "        x_combined = torch.cat(processed_features, dim=1)\n",
        "        return self.classifier(x_combined)\n",
        "\n",
        "# Define each client's input feature dimension\n",
        "client_feature_dims = [client1_train_loader.dataset.tensors[0].shape[1],\n",
        "                       client2_train_loader.dataset.tensors[0].shape[1],\n",
        "                       client3_train_loader.dataset.tensors[0].shape[1]]\n",
        "\n",
        "server_model = FraudDetectionModel(client_feature_dims)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "io7wABN7SCjA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "j41BYuiuJaVW",
        "outputId": "7bf58b7b-d598-41f6-9917-beb3c5b7d15d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Federated Training Round 1 ---\n",
            "\n",
            "Training on Client 1...\n",
            "Client 1, Epoch 1/10, Loss: 0.2607587755688793\n",
            "Client 1, Epoch 2/10, Loss: 0.23694331872894722\n",
            "Client 1, Epoch 3/10, Loss: 0.2320861233454523\n",
            "Client 1, Epoch 4/10, Loss: 0.2299233409046117\n",
            "Client 1, Epoch 5/10, Loss: 0.22774593726829948\n",
            "Client 1, Epoch 6/10, Loss: 0.22666801666557915\n",
            "Client 1, Epoch 7/10, Loss: 0.22558766350745682\n",
            "Client 1, Epoch 8/10, Loss: 0.22467276880832152\n",
            "Client 1, Epoch 9/10, Loss: 0.2240871781697265\n",
            "Client 1, Epoch 10/10, Loss: 0.22358732917550667\n",
            "\n",
            "Training on Client 2...\n",
            "Client 2, Epoch 1/10, Loss: 0.010454719487861076\n",
            "Client 2, Epoch 2/10, Loss: 0.0036688045497278634\n",
            "Client 2, Epoch 3/10, Loss: 0.003496953032618586\n",
            "Client 2, Epoch 4/10, Loss: 0.0033765184234126927\n",
            "Client 2, Epoch 5/10, Loss: 0.003234263747394942\n",
            "Client 2, Epoch 6/10, Loss: 0.003146052499884174\n",
            "Client 2, Epoch 7/10, Loss: 0.0031119513966017718\n",
            "Client 2, Epoch 8/10, Loss: 0.003033942130806493\n",
            "Client 2, Epoch 9/10, Loss: 0.003042435788247436\n",
            "Client 2, Epoch 10/10, Loss: 0.0029259378426393537\n",
            "\n",
            "Training on Client 3...\n",
            "Client 3, Epoch 1/10, Loss: 0.026126633795995873\n",
            "Client 3, Epoch 2/10, Loss: 0.02335873445423766\n",
            "Client 3, Epoch 3/10, Loss: 0.023018639268032517\n",
            "Client 3, Epoch 4/10, Loss: 0.02279810610812082\n",
            "Client 3, Epoch 5/10, Loss: 0.022682681961310167\n",
            "Client 3, Epoch 6/10, Loss: 0.022501781472223058\n",
            "Client 3, Epoch 7/10, Loss: 0.022386822315104832\n",
            "Client 3, Epoch 8/10, Loss: 0.022317500540176064\n",
            "Client 3, Epoch 9/10, Loss: 0.022192982986026233\n",
            "Client 3, Epoch 10/10, Loss: 0.02218252148777155\n",
            "Global Model Updated!\n",
            "\n",
            "--- Federated Training Round 2 ---\n",
            "\n",
            "Training on Client 1...\n",
            "Client 1, Epoch 1/10, Loss: 0.23845828731597069\n",
            "Client 1, Epoch 2/10, Loss: 0.22925185353695843\n",
            "Client 1, Epoch 3/10, Loss: 0.22673742406468597\n",
            "Client 1, Epoch 4/10, Loss: 0.22502099655966964\n",
            "Client 1, Epoch 5/10, Loss: 0.224373688873016\n",
            "Client 1, Epoch 6/10, Loss: 0.22335389799395525\n",
            "Client 1, Epoch 7/10, Loss: 0.2226828074919887\n",
            "Client 1, Epoch 8/10, Loss: 0.2218959838725812\n",
            "Client 1, Epoch 9/10, Loss: 0.22149537911154468\n",
            "Client 1, Epoch 10/10, Loss: 0.22127128222857662\n",
            "\n",
            "Training on Client 2...\n",
            "Client 2, Epoch 1/10, Loss: 0.003849051479236319\n",
            "Client 2, Epoch 2/10, Loss: 0.0033110429710204617\n",
            "Client 2, Epoch 3/10, Loss: 0.0032360807088263346\n",
            "Client 2, Epoch 4/10, Loss: 0.003081482178904571\n",
            "Client 2, Epoch 5/10, Loss: 0.0031108340377689653\n",
            "Client 2, Epoch 6/10, Loss: 0.0030132851904665706\n",
            "Client 2, Epoch 7/10, Loss: 0.002907836342575642\n",
            "Client 2, Epoch 8/10, Loss: 0.0029218569779086477\n",
            "Client 2, Epoch 9/10, Loss: 0.002880866072292562\n",
            "Client 2, Epoch 10/10, Loss: 0.002875319495206554\n",
            "\n",
            "Training on Client 3...\n",
            "Client 3, Epoch 1/10, Loss: 0.0234328466712029\n",
            "Client 3, Epoch 2/10, Loss: 0.022648461148141473\n",
            "Client 3, Epoch 3/10, Loss: 0.02248720833619667\n",
            "Client 3, Epoch 4/10, Loss: 0.022341659284136728\n",
            "Client 3, Epoch 5/10, Loss: 0.022248423740561987\n",
            "Client 3, Epoch 6/10, Loss: 0.02215444773796571\n",
            "Client 3, Epoch 7/10, Loss: 0.022049643255565694\n",
            "Client 3, Epoch 8/10, Loss: 0.021981544565115786\n",
            "Client 3, Epoch 9/10, Loss: 0.02188591421021688\n",
            "Client 3, Epoch 10/10, Loss: 0.02182516254667897\n",
            "Global Model Updated!\n",
            "\n",
            "--- Federated Training Round 3 ---\n",
            "\n",
            "Training on Client 1...\n",
            "Client 1, Epoch 1/10, Loss: 0.23136613403568748\n",
            "Client 1, Epoch 2/10, Loss: 0.22572354570153436\n",
            "Client 1, Epoch 3/10, Loss: 0.2236514216019733\n",
            "Client 1, Epoch 4/10, Loss: 0.22263985498556388\n",
            "Client 1, Epoch 5/10, Loss: 0.22227814261120674\n",
            "Client 1, Epoch 6/10, Loss: 0.22161189201172066\n",
            "Client 1, Epoch 7/10, Loss: 0.2210886763835739\n",
            "Client 1, Epoch 8/10, Loss: 0.22069814872112994\n",
            "Client 1, Epoch 9/10, Loss: 0.22043806698829993\n",
            "Client 1, Epoch 10/10, Loss: 0.2201586874796481\n",
            "\n",
            "Training on Client 2...\n",
            "Client 2, Epoch 1/10, Loss: 0.0034828749147576017\n",
            "Client 2, Epoch 2/10, Loss: 0.0031404613369275377\n",
            "Client 2, Epoch 3/10, Loss: 0.003046164795970683\n",
            "Client 2, Epoch 4/10, Loss: 0.0029971211038071813\n",
            "Client 2, Epoch 5/10, Loss: 0.002854542884201185\n",
            "Client 2, Epoch 6/10, Loss: 0.00286115720693292\n",
            "Client 2, Epoch 7/10, Loss: 0.002816982897286659\n",
            "Client 2, Epoch 8/10, Loss: 0.0027756660131715146\n",
            "Client 2, Epoch 9/10, Loss: 0.0027988828614949213\n",
            "Client 2, Epoch 10/10, Loss: 0.0026873387545011994\n",
            "\n",
            "Training on Client 3...\n",
            "Client 3, Epoch 1/10, Loss: 0.02262537625868685\n",
            "Client 3, Epoch 2/10, Loss: 0.02219179167000557\n",
            "Client 3, Epoch 3/10, Loss: 0.022096049980240593\n",
            "Client 3, Epoch 4/10, Loss: 0.021979526519531885\n",
            "Client 3, Epoch 5/10, Loss: 0.021837012659570172\n",
            "Client 3, Epoch 6/10, Loss: 0.021751184554728544\n",
            "Client 3, Epoch 7/10, Loss: 0.02165268404717032\n",
            "Client 3, Epoch 8/10, Loss: 0.02160239130394149\n",
            "Client 3, Epoch 9/10, Loss: 0.02155031846011455\n",
            "Client 3, Epoch 10/10, Loss: 0.021466363752857768\n",
            "Global Model Updated!\n"
          ]
        }
      ],
      "source": [
        "# Function to train a model locally on a client's dataset\n",
        "def train_local_model(model, train_loader, client_idx, epochs=10, lr=0.001):\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Ensure tensors are detached to avoid computation graph retention\n",
        "            X_batch = X_batch.detach()\n",
        "            y_batch = y_batch.detach()\n",
        "\n",
        "            # Create zero inputs for other clients\n",
        "            input_list = [X_batch if i == client_idx else torch.zeros_like(X_batch).detach()\n",
        "                          for i in range(model.num_clients)]\n",
        "\n",
        "            y_pred = model(input_list).squeeze()\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "\n",
        "            loss.backward(retain_graph=False)  # Ensure the graph is not retained\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Client {client_idx+1}, Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "    return model.state_dict()\n",
        "\n",
        "\n",
        "# Function to aggregate weights from multiple clients\n",
        "def federated_averaging(global_model, local_weights):\n",
        "    new_state_dict = copy.deepcopy(global_model.state_dict())\n",
        "\n",
        "    for key in new_state_dict.keys():\n",
        "        new_state_dict[key] = torch.stack([local_weights[i][key] for i in range(len(local_weights))], dim=0).mean(dim=0)\n",
        "\n",
        "    global_model.load_state_dict(new_state_dict)\n",
        "    return global_model\n",
        "\n",
        "# Federated Learning Training\n",
        "num_rounds = 3\n",
        "train_loaders = [client1_train_loader, client2_train_loader, client3_train_loader]\n",
        "\n",
        "for round_num in range(num_rounds):\n",
        "    print(f\"\\n--- Federated Training Round {round_num+1} ---\")\n",
        "    local_weights = []\n",
        "\n",
        "    for i, train_loader in enumerate(train_loaders):\n",
        "        print(f\"\\nTraining on Client {i+1}...\")\n",
        "        local_model = copy.deepcopy(server_model)\n",
        "        local_weights.append(train_local_model(local_model, train_loader, i))  # Train & store weights\n",
        "\n",
        "    # Aggregate local weights into global model\n",
        "    server_model = federated_averaging(server_model, local_weights)\n",
        "    print(\"Global Model Updated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the Model(optional)"
      ],
      "metadata": {
        "id": "uDZTazsJXw_r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iQ4LTz-RZYt"
      },
      "outputs": [],
      "source": [
        "#torch.save(server_model.state_dict(), \"/content/drive/My Drive/Fraud Detection/server_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42u1l3D2R0z-",
        "outputId": "5570f1ce-18ec-47d5-b32f-cf7c1072097d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#server_model.load_state_dict(torch.load(\"/content/drive/My Drive/Fraud Detection/server_model.pth\"))\n",
        "#server_model.eval()  # Set the model to evaluation mode for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation on Test Data"
      ],
      "metadata": {
        "id": "YYVNbcXuX4qE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMvcRLDS9hFO"
      },
      "outputs": [],
      "source": [
        "test_loader = DataLoader(TensorDataset(X_test_combined, y_test_combined), batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8Rx1pIKw1u6",
        "outputId": "451562b5-6b87-4e12-bde0-5ce3ab038873"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9778\n",
            "Precision: 0.0557\n",
            "Recall: 0.0461\n",
            "F1-score: 0.0504\n",
            "PR-AUC: 0.0210\n",
            "Roc-AUC score: 0.6070\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score, precision_recall_curve, roc_curve, roc_auc_score\n",
        "import torch\n",
        "\n",
        "# Function to evaluate the model on a given test dataset\n",
        "def evaluate(model, test_loaders):\n",
        "    model.eval()\n",
        "\n",
        "    # Iterate over each client and evaluate on their test data\n",
        "    for i, test_loader in enumerate(test_loaders):\n",
        "        y_true, y_pred_probs = [], []\n",
        "        #correct, total = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                # Ensure tensors are detached to avoid computation graph retention\n",
        "                X_batch = X_batch.detach()\n",
        "                y_batch = y_batch.detach()\n",
        "\n",
        "                # Create zero inputs for other clients, similar to the training phase\n",
        "                input_list = [X_batch if i == client_idx else torch.zeros_like(X_batch).detach()\n",
        "                              for client_idx in range(model.num_clients)]\n",
        "\n",
        "                y_pred = model(input_list).squeeze()  # Forward pass\n",
        "                y_pred_probs.extend(y_pred.cpu().numpy())  # Store predicted probabilities\n",
        "                y_true.extend(y_batch.cpu().numpy())  # Store actual labels\n",
        "\n",
        "                # predictions = (y_pred >= 0.5).float()  # Convert predictions to binary values\n",
        "                # correct += (predictions == y_batch).sum().item()  # Count correct predictions\n",
        "                # total += y_batch.size(0)\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        y_true = np.array(y_true)\n",
        "        y_pred_probs = np.array(y_pred_probs)\n",
        "\n",
        "        # Compute and print accuracy, precision, recall, F1-score, and PR-AUC\n",
        "        # accuracy = correct / total\n",
        "        # print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Threshold tuning: find best threshold based on F1-score\n",
        "        precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_probs)\n",
        "        f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
        "        best_index = np.argmax(f1_scores)\n",
        "        best_threshold = thresholds[best_index] if best_index < len(thresholds) else 0.5\n",
        "\n",
        "        # Compute Precision, Recall, F1-score\n",
        "        y_pred_binary = (y_pred_probs >= best_threshold).astype(int)\n",
        "        accuracy = (y_pred_binary == y_true).mean()\n",
        "        precision = precision_score(y_true, y_pred_binary)\n",
        "        recall = recall_score(y_true, y_pred_binary)\n",
        "        f1 = f1_score(y_true, y_pred_binary)\n",
        "        pr_auc = average_precision_score(y_true, y_pred_probs)  # PR-AUC Score\n",
        "        roc_auc = roc_auc_score(y_true, y_pred_probs)  # ROC-AUC Score\n",
        "\n",
        "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-score: {f1:.4f}\")\n",
        "        print(f\"PR-AUC: {pr_auc:.4f}\")\n",
        "        print(f\"Roc-AUC score: {roc_auc:.4f}\")\n",
        "\n",
        "\n",
        "test_loaders = [test_loader]\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate(server_model, test_loaders)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}